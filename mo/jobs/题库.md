| ID   | 题目                                        | 难度 | 答案要点                                            |
| :--- | :------------------------------------------ | :--- | :-------------------------------------------------- |
| 1    | 逻辑回归的损失函数形式                      | 简单 |                                                     |
| 2    | 逻辑回归的梯度下降的原理&推导               | 中等 | 理解逻辑回归损失函数求导方法&梯度所示物理意义       |
| 3    | gbdt，xgb， lgb的区别                       | 中等 |                                                     |
| 4    | xgb 和 rf的区别，哪个方差高、哪个偏差大     | 中等 |                                                     |
| 5    | 深度学习为什么需要有激活层                  | 简单 |                                                     |
| 6    | Batch Normalization 的原理&作用             | 中等 |                                                     |
| 7    | word2vec的原理                              | 中等 |                                                     |
| 8    | lstm三个门的含义并画出结构图                | 中等 |                                                     |
| 9    | 正则化的方法有哪些                          | 中等 |                                                     |
| 10   | softmax的损失函数为什么不用mse而用交叉熵    | 稍难 | 损失函数用mse导数在绝对误差过大时可能变得特别平缓； |
| 11   | lstm相对于普通rnn为什么能够缓解梯度消失问题 | 中等 |                                                     |

 

